# -*- coding: utf-8 -*-
"""Personalized News Feed Agent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HX81R6L6Kkr2Lt3-ZGOpJD8X8ug5uPAt
"""

!pip install requests pandas

import requests
import pandas as pd
import random

API_KEY = '88539f10ee044627a077255d12016436'  # Your NewsAPI key here

def fetch_news():
    url = f"https://newsapi.org/v2/top-headlines?language=en&pageSize=10&apiKey={API_KEY}"
    response = requests.get(url)

    if response.status_code == 200:
        articles = response.json()['articles']

        news_data = []
        for article in articles:
            news_data.append({
                'title': article['title'],
                'description': article['description'],
                'content': article['content'],
                'url': article['url'],
                'publishedAt': article['publishedAt'],
            })

        return news_data
    else:
        print(f"Error fetching news: {response.status_code}")
        return []

# Fetch news articles
news = fetch_news()

for i, article in enumerate(news[:3], start=1):
    print(f"Article {i}:")
    print(f"Title: {article['title']}")
    print(f"Description: {article['description']}")
    print(f"URL: {article['url']}\n")

# Example user IDs
user_ids = [101, 102, 103, 104]

# Simulate interactions for each user
interactions = []

for user_id in user_ids:
    for article in news:
        # Simulate random interactions (click/like)
        interaction_type = random.choice(['click', 'like'])
        time_spent = random.randint(20, 120)  # Random time spent in seconds

        interactions.append({
            'user_id': user_id,
            'article_id': article['url'],  # Using article URL as unique ID
            'interaction': interaction_type,
            'time_spent': time_spent
        })

# Display first 5 simulated interactions
for interaction in interactions[:5]:
    print(interaction)

df_interactions = pd.DataFrame(interactions)

# Display the first few rows of interactions
df_interactions.head()

df_interactions.to_csv('/content/user_interactions.csv', index=False)

# Aggregate articles interacted with by each user
user_profiles = {}

for interaction in interactions:
    user_id = interaction['user_id']
    article_url = interaction['article_id']
    content = next((article['content'] for article in news if article['url'] == article_url), '')

    # Ensure content is not None, if None, use an empty string
    if content is None:
        content = ''

    if user_id not in user_profiles:
        user_profiles[user_id] = []
    user_profiles[user_id].append(content)

# Use CountVectorizer to extract features for each user's interactions
vectorizer = CountVectorizer(stop_words='english')
user_profiles_vectorized = {}

for user_id, articles in user_profiles.items():
    user_profile_text = " ".join(articles)
    user_profiles_vectorized[user_id] = vectorizer.fit_transform([user_profile_text]).toarray()

# Display user profile vectors
for user_id, profile in user_profiles_vectorized.items():
    print(f"User {user_id} Profile: {profile}")

from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# Load pre-trained Sentence-BERT model
model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# Step 1: Encode article content into embeddings using Sentence-BERT
article_embeddings = model.encode([article['content'] for article in valid_articles])

# Step 2: For each user, calculate the similarity between their profile and the articles
personalized_recommendations = {}

for user_id, profile in user_profiles_vectorized.items():
    # Generate embedding for user profile (combine user-interacted articles)
    user_profile_text = " ".join(user_profiles[user_id])  # Combine articles into a single text
    user_profile_embedding = model.encode(user_profile_text)

    # Step 3: Calculate cosine similarity between user profile and articles
    user_similarity = cosine_similarity([user_profile_embedding], article_embeddings)

    # Step 4: Get the indices of the top 5 most similar articles
    top_article_indices = user_similarity[0].argsort()[-5:][::-1]

    # Store recommendations for this user
    recommended_articles = []
    for idx in top_article_indices:
        article = valid_articles[idx]
        recommended_articles.append({
            'title': article['title'],
            'url': article['url'],
            'description': article['description']
        })

    personalized_recommendations[user_id] = recommended_articles

# Display personalized recommendations for each user
for user_id, recommendations in personalized_recommendations.items():
    print(f"Personalized News Feed for User {user_id}:")
    for article in recommendations:
        print(f"Title: {article['title']}\nURL: {article['url']}\nDescription: {article['description']}\n")

# Simulate new interactions (click, like)
new_interactions = [
    {'user_id': 101, 'article_id': news[0]['url'], 'interaction': 'click', 'time_spent': 60},
    {'user_id': 102, 'article_id': news[1]['url'], 'interaction': 'like', 'time_spent': 120}
]

# Update user profiles with new interactions
for interaction in new_interactions:
    user_id = interaction['user_id']
    article_url = interaction['article_id']
    content = next((article['content'] for article in news if article['url'] == article_url), '')

    if content:
        user_profiles[user_id].append(content)

# Re-vectorize user profiles (optional: vectorize every time profiles change)
user_profiles_vectorized = {}

for user_id, articles in user_profiles.items():
    user_profile_text = " ".join(articles)
    user_profiles_vectorized[user_id] = model.encode([user_profile_text])

# Now, generate new personalized recommendations with updated user profiles
# (Same recommendation process as before)

import matplotlib.pyplot as plt
from sklearn.metrics.pairwise import cosine_similarity

# Visualize top 5 recommended articles for User 101
user_id = 101
recommended_articles = personalized_recommendations[user_id]

# Get the top 5 articles and their similarity scores
article_titles = [article['title'] for article in recommended_articles]
# Use 'description' instead of 'content' for similarity calculation
similarity_scores = [cosine_similarity([model.encode(" ".join(user_profiles[user_id]))], [model.encode(article['description'] or "")])[0][0] for article in recommended_articles]

plt.figure(figsize=(10, 6))
plt.barh(article_titles, similarity_scores)
plt.xlabel('Cosine Similarity Score')
plt.title(f"Top 5 Recommended Articles for User {user_id}")
plt.show()

from google.colab import files
files.download('user_interactions.csv')